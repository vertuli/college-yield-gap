{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from os.path import isfile\n",
    "from requests import get\n",
    "from IPython.core.display import clear_output\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14) \"\n",
    "           \"AppleWebKit/605.1.15 (KHTML, like Gecko) \"\n",
    "           \"Version/12.0 Safari/605.1.15\"}\n",
    "\n",
    "BASE_URL_1 = \"https://www.collegedata.com/cs/data/college/college_pg0\"\n",
    "BASE_URL_2 = \"_tmpl.jhtml?schoolId=\"\n",
    "\n",
    "EMPTY_TITLE = \"Retrieve a Saved Search\"\n",
    "\n",
    "SCHOOL_ID_START = 1\n",
    "SCHOOL_ID_END = 20\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "\n",
    "def get_clean_string(tag):\n",
    "    child_tags = tag.find_all(True)\n",
    "    if child_tags:\n",
    "        for child_tag in tag.find_all(True):\n",
    "            if child_tag.name == 'div':\n",
    "                child_tag.unwrap()\n",
    "            # else:\n",
    "                child_tag.decompose()     \n",
    "    value = \" \".join(tag.stripped_strings)\n",
    "    \n",
    "    return value\n",
    "\n",
    "\n",
    "def scrape_row(tr_tag):\n",
    "    scraped_row = {}\n",
    "    \n",
    "    th_tags = tr_tag.find_all('th')\n",
    "    if not th_tags:\n",
    "        return None\n",
    "    \n",
    "    th_tag = th_tags[0]\n",
    "    label = get_clean_string(th_tag)\n",
    "    if not label:\n",
    "        return None\n",
    "    \n",
    "    values = []\n",
    "    parent = th_tag.parent\n",
    "    if not parent:\n",
    "        return None\n",
    "    \n",
    "    td_tags = parent.find_all('td')\n",
    "    if not td_tags:\n",
    "        return None\n",
    "    \n",
    "    for td_tag in td_tags:\n",
    "        value = get_clean_string(td_tag)\n",
    "        values.append(value)\n",
    "    \n",
    "    scraped_row[label] = values\n",
    "    return scraped_row\n",
    "\n",
    "    \n",
    "def scrape_table(thead_tag):\n",
    "    # Get the column labels from <thead>, if they exist.\n",
    "    # If they do not, use a number label placeholder instead.\n",
    "    column_labels = []\n",
    "    thead_td_tags = thead_tag.find_all('td')\n",
    "    if not thead_td_tags:\n",
    "        return None\n",
    "    for thead_td_tag in thead_td_tags:\n",
    "        i = 0\n",
    "        if thead_td_tag.string:\n",
    "            column_label = thead_td_tag.string\n",
    "        else:\n",
    "            column_label = None\n",
    "        while column_label in column_labels:\n",
    "            if column_label:\n",
    "                column_label += '*'\n",
    "            else:\n",
    "                column_label = '*'\n",
    "        column_labels.append(column_label)\n",
    "\n",
    "    # Get the <th> strings in the <tbody>.\n",
    "    if thead_tag.parent:\n",
    "        if not thead_tag.parent.tbody:\n",
    "            return None\n",
    "        tbody_tr_tags = thead_tag.parent.tbody.find_all('tr')\n",
    "        if not tbody_tr_tags:\n",
    "            return None\n",
    "    \n",
    "    scraped_table = {}\n",
    "    # Scrape the rows of each table.\n",
    "    for tr_tag in tbody_tr_tags:\n",
    "        scraped_row = {}\n",
    "        raw_scraped_row = scrape_row(tr_tag)\n",
    "        if raw_scraped_row:\n",
    "            all_raw_values = []\n",
    "            for raw_value in raw_scraped_row.values():\n",
    "                all_raw_values += raw_value\n",
    "            unique_vals = set(all_raw_values)\n",
    "                \n",
    "            for raw_label, raw_values in raw_scraped_row.items():\n",
    "                for i, raw_value in enumerate(raw_values):\n",
    "                    if (len(unique_vals) == 2) and ('X' in unique_vals):\n",
    "                        if raw_value == 'X':\n",
    "                            scraped_row[raw_label] = column_labels[i]\n",
    "                    else:\n",
    "                        column_label = column_labels[i]\n",
    "                        label = raw_label\n",
    "                        if column_label:\n",
    "                            label = raw_label + \" - \" + column_label\n",
    "                        scraped_row[label] = raw_value\n",
    "                        \n",
    "        scraped_table.update(scraped_row)\n",
    "        \n",
    "    return scraped_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_soup(soup):\n",
    "    # Relabel and remove some data from soup before scraping.\n",
    "    \n",
    "    # Relabel the variable '{CITY} Population' tag with a constant label.\n",
    "    tags = soup.find_all(string = re.compile('Population'))\n",
    "    if tags:\n",
    "        for tag in tags:\n",
    "            tag.string.replace_with('City Population')\n",
    "    \n",
    "    \n",
    "    # Remove data that is either extraneous or irrelevant for analysis.\n",
    "    \n",
    "    # Remove entire sections.\n",
    "    remove_ids=['section10','section19']\n",
    "    tags = soup.find_all(id=remove_ids)\n",
    "    if tags:\n",
    "        for tag in tags:\n",
    "            tag.decompose()\n",
    "            \n",
    "    # Remove tables.\n",
    "    captions = ['Selection of Students',\n",
    "                'Undergraduate Majors',\n",
    "                'Intercollegiate Sports Offered']\n",
    "    regexps = [re.compile(caption) for caption in captions]\n",
    "    tags = soup.find_all('caption', string=regexps)\n",
    "    if tags:\n",
    "        for tag in tags:\n",
    "            tag.parent.decompose()\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_page(school_id, page_id):    \n",
    "    scraped_page = {}\n",
    "    \n",
    "    # Build URL and send request.\n",
    "    url = BASE_URL_1 + str(page_id) + BASE_URL_2 + str(school_id)\n",
    "    result = get(url, headers = HEADERS)\n",
    "    \n",
    "    # Print status update.\n",
    "    print(\"Scraping {}\".format(result.url))\n",
    "    \n",
    "    # Save School Id.\n",
    "    scraped_page['School Id'] = school_id\n",
    "    \n",
    "    error = None\n",
    "    # Check request errors.\n",
    "    if result.status_code != 200:\n",
    "        error = \"Page {}: Status Code {}\".format(page_id, result.status_code)\n",
    "        print(\"ERROR {} with {}\".format(error, result.url))\n",
    "        scraped_page['Error'] = error\n",
    "        return scraped_page\n",
    "\n",
    "    soup = BeautifulSoup(result.text, \"lxml\")\n",
    "    \n",
    "    # Scrape name.\n",
    "    scraped_page['Name'] = soup.h1.string\n",
    "\n",
    "    # Check errors in returned HTML soup.\n",
    "    if not soup.h1:\n",
    "        error = \"Page {}: No heading tag.\".format(page_id)\n",
    "        print(\"ERROR {} with {}\".format(error, result.url))\n",
    "        scraped_page['Error'] = error\n",
    "        return scraped_page\n",
    "\n",
    "    if soup.h1 and soup.h1.string == EMPTY_TITLE:\n",
    "        error = \"Page {}: No school data.\".format(page_id)\n",
    "        print(\"ERROR {} with {}\".format(error, result.url))\n",
    "        scraped_page['Error'] = error\n",
    "        return scraped_page\n",
    "\n",
    "    if not soup.find(string='Content END'):\n",
    "        error = \"Page {}: Page not fully loaded.\".format(page_id)\n",
    "        print(\"ERROR {} with {}\".format(error, result.url))\n",
    "        scraped_page['Error'] = error\n",
    "        return scraped_page\n",
    "\n",
    "    # Clean the soup.\n",
    "    preprocess_soup(soup)\n",
    "    \n",
    "    # Scrape data from tables with <thead> and <tbody> tags, if any.\n",
    "    tags = soup.find(id='tabcontwrap').find_all('thead')\n",
    "    if tags:\n",
    "        for tag in tags:\n",
    "            scraped_table = scrape_table(tag)\n",
    "            \n",
    "            scraped_page.update(scraped_table)\n",
    "        \n",
    "            # Delete table from soup.\n",
    "            tag.parent.decompose()\n",
    "        \n",
    "\n",
    "    # Scrape remaining table rows.\n",
    "    tags = soup.find(id='tabcontwrap').find_all('tr')\n",
    "    if tags:\n",
    "        for tag in tags:\n",
    "            scraped_row = scrape_row(tag)\n",
    "            \n",
    "            if scraped_row:\n",
    "                for label, values in scraped_row.items():\n",
    "                    scraped_row[label] = values[0]\n",
    "            \n",
    "                scraped_page.update(scraped_row)\n",
    "\n",
    "    # Clear status update.\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    return scraped_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_dicts(dict_1, dict_2):\n",
    "    if not dict_1 and not dict_2:\n",
    "        return None\n",
    "    if not dict_1:\n",
    "        return dict_2\n",
    "    if not dict_2:\n",
    "        return dict_1\n",
    "    \n",
    "    for dict_2_key, dict_2_value in dict_2.items():\n",
    "        \n",
    "        if dict_2_key not in dict_1.keys():\n",
    "            dict_1[dict_2_key] = dict_2_value\n",
    "        \n",
    "        else:\n",
    "            dict_1_value = dict_1[dict_2_key]\n",
    "            if dict_1_value != dict_2_value:\n",
    "                \n",
    "                new_key = dict_2_key\n",
    "                while new_key in dict_1.keys():\n",
    "                    new_key += '*'\n",
    "                dict_1[new_key] = dict_2_value\n",
    "                \n",
    "    return dict_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def scrape_school(school_id):\n",
    "    page_ids = np.arange(1, 7)\n",
    "    scraped_pages = [scrape_page(school_id, page_id) for page_id in page_ids]\n",
    "    \n",
    "    scraped_school = {}\n",
    "    for scraped_page in scraped_pages:\n",
    "        scraped_school = merge_dicts(scraped_school, scraped_page)\n",
    "    \n",
    "    return scraped_school"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://www.collegedata.com/cs/data/college/college_pg06_tmpl.jhtml?schoolId=9\n"
     ]
    }
   ],
   "source": [
    "scraped = {school_id: scrape_school(school_id) for school_id in range(6, 10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4 entries, 6 to 9\n",
      "Columns: 234 entries, Name to Disciplines Pursued\n",
      "dtypes: object(234)\n",
      "memory usage: 7.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(scraped, orient='index')\n",
    "df.index = df['School Id']\n",
    "df = df.drop(columns='School Id')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
