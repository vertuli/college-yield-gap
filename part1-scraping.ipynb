{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from os.path import isfile\n",
    "from requests import get\n",
    "from IPython.core.display import clear_output\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14) \"\n",
    "           \"AppleWebKit/605.1.15 (KHTML, like Gecko) \"\n",
    "           \"Version/12.0 Safari/605.1.15\"}\n",
    "\n",
    "BASE_URL_1 = \"https://www.collegedata.com/cs/data/college/college_pg0\"\n",
    "BASE_URL_2 = \"_tmpl.jhtml?schoolId=\"\n",
    "\n",
    "EMPTY_H1_HEADING = \"Retrieve a Saved Search\"\n",
    "\n",
    "SCHOOL_ID_START = 1\n",
    "SCHOOL_ID_END = 5000\n",
    "\n",
    "COLLEGEDATA_RAW_PATH = \"data/collegedata_raw.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Main idea\n",
    "\n",
    "Scrape a school, save the scraped data to CSV, repeat for all schools.\n",
    "\n",
    "If some scraping has already been done, pick up where we last left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_collegedata(start = SCHOOL_ID_START, stop = SCHOOL_ID_END):\n",
    "    # If a previously scraped csv exists, start the scraper where it left off.\n",
    "    if isfile(COLLEGEDATA_RAW_PATH):\n",
    "        ids = pd.read_csv(COLLEGEDATA_RAW_PATH, usecols = 'SchoolId')\n",
    "        start = ids.max() + 1\n",
    "    \n",
    "    # Scrape and save each school.\n",
    "    for school_id in range(start, stop + 1):\n",
    "        school = scrape_school(school_id)\n",
    "        save_to_csv(school)\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Saving the scraped school\n",
    "When saving, if a scraped file already exists, append the scraped school as a row. If the file doesn't exist (this is the first school), write column headers during the save, too.\n",
    "\n",
    "If there was no school data scraped, then do nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_to_csv(school):\n",
    "    if not school:\n",
    "        return None\n",
    "    \n",
    "    df = pd.DataFrame(school, index = school['SchoolId'])\n",
    "    \n",
    "    if isfile(COLLEGEDATA_RAW_PATH):\n",
    "        df.to_csv(COLLEGEDATA_RAW_PATH, mode = 'a+', header = False)\n",
    "    else:\n",
    "        df.to_csv(PATH, mode = 'a+', header = True, index_label = 'SchoolId')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Scraping a school\n",
    "\n",
    "Great, so all we have to do now is figure out how to scrape a school.\n",
    "\n",
    "It turns out, a school's worth of information on CollegeData.com is actually six pages of data. To scrape a school, we must scrape all six of these pages.\n",
    "\n",
    "If any of those pages fail to scrape, we'll abandon the whole thing and return nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def scrape_school(school_id):\n",
    "    school = {}\n",
    "    for page_id in range(1, 7):\n",
    "        page = scrape_page(school_id, page_id)\n",
    "        if not page:\n",
    "            return {}\n",
    "        school.update(page)\n",
    "    school['SchoolId'] = school_id\n",
    "    return school"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Scraping a page\n",
    "\n",
    "To scrape a page, we'll need to get the Beautiful Soup object containing all the page's raw HTML.\n",
    "\n",
    "Then we'll \"clean\" the soup with some preprocessing to make the extraction a bit easier.\n",
    "\n",
    "We'll scrape the data coming from the common \"rows\" part of the pages that look like this:\n",
    "\n",
    "And we'll separately handle the data coming from these \"tables\" parts of the pages that look like this:\n",
    "\n",
    "We'll return what we found (which could be nothing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_page(school_id, page_id):\n",
    "    page = {}\n",
    "    soup = get_soup(school_id, page_id)\n",
    "    cleaned_soup = clean_soup(soup)\n",
    "    \n",
    "    page.update(scrape_rows(cleaned_soup))\n",
    "    page.update(scrape_tables(cleaned_soup))\n",
    "    return page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Getting the soup\n",
    "\n",
    "We'll need to construct the URL for the page given the school's id and the current page's id.\n",
    "\n",
    "If we get a status code other than 200, we'll return None. Otherewise, we'll use Beautiful Soup to convert the response to a soup object.\n",
    "\n",
    "If for some reason the page doesn't have a `<h1>` header tag (it should!), then this isn't a normal CollegeData page with useful data, so we'll stop and return None.\n",
    "\n",
    "If the page does have an `<h1>` tag but it says \"Retrieve a Saved Search\", we know we've hit a CollegeData page corresponding to no school, and since there is no useful data, we'll stop and return None.\n",
    "\n",
    "A fully loaded normal page with CollegeData content should, toward the end of the document, contain an HTML comment tag that says 'Content END'. If for some reason we were not served the entire page, we won't see this tag. If that happens, we should stop and return None.\n",
    "\n",
    "If the request and soup passed those three checks, we'll return the soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_soup(school_id, page_id):\n",
    "    \n",
    "    # Build URL and send request.\n",
    "    url = BASE_URL_1 + str(page_id) + BASE_URL_2 + str(school_id)\n",
    "    print(\"Scraping {}\".format(url))\n",
    "    result = get(url, headers = HEADERS)\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    # Abort if received an unusual status code.\n",
    "    if result.status_code != 200:\n",
    "        # TO DO: Add a proper way to handle or at least record this error.\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(result.text, \"lxml\")\n",
    "    \n",
    "    # Abort if page does not have a <h1> header.\n",
    "    if not soup.h1:\n",
    "        # TO DO: Add a proper way to handle or at least record this error.\n",
    "        return None\n",
    "    \n",
    "    # Abort if <h1> header matches that of the error page with no school data.\n",
    "    if soup.h1.string == EMPTY_H1_HEADING:\n",
    "        # TO DO: Add a proper way to handle or at least record this error.\n",
    "        return None\n",
    "    \n",
    "    # Abort if the entire page HTML did not load.\n",
    "    if not soup.find(string = 'Content END'):\n",
    "        # TO DO: Add a proper way to handle or at least record this error.\n",
    "        return None\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleaning the soup\n",
    "\n",
    "First, if there is no soup, then return None.\n",
    "\n",
    "We're going to delete some sections we don't need.\n",
    "\n",
    "We also want to rename some labels that are either ambiguous or shared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_soup(soup):\n",
    "    if soup:\n",
    "        \n",
    "    \n",
    "        soup = delete_tags(soup)\n",
    "        soup = rename_labels(soup)\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Deleting parts of the HTML\n",
    "First, we'll see if the page has this section (found by the unique  `id = 'section19'` in the HTML):\n",
    "\n",
    "\n",
    "If it does, we'll delete it because...\n",
    "\n",
    "In addition to that section, we'll also delete these sections:\n",
    "\n",
    "\n",
    "I decided this because ...\n",
    "\n",
    "These sections can be identified and removed by the unique strings in their `<caption>` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_tags(soup):\n",
    "    # Delete certain div sections by their id tags.\n",
    "    div_ids = ['section19']\n",
    "    tags = soup.find_all(id = div_ids)\n",
    "    if tags:\n",
    "        for tag in tags:\n",
    "            tag.decompose()\n",
    "            \n",
    "    # Delete duplicate values.\n",
    "    tag = soup.find('div', id = 'section26')\n",
    "    if tag:\n",
    "        strings = ['All Undergraduates','Women','Men']\n",
    "        for string in strings:\n",
    "            regex = re.compile(string)\n",
    "            tag.find_next('th', string = regex).parent.decompose()\n",
    "            \n",
    "            \n",
    "    # Delete certain redundant tables by their caption strings.\n",
    "    captions = ['Selection of Students',\n",
    "                'Grade Point Average of Enrolled Freshmen',\n",
    "                'SAT Scores of Enrolled Freshmen',\n",
    "                'ACT Scores of Enrolled Freshmen',\n",
    "                'Financial Aid Office',\n",
    "                'Undergraduate Majors',\n",
    "                'Intercollegiate Sports Offered']\n",
    "    regexs = [re.compile(caption) for caption in captions]\n",
    "    tags = soup.find_all('caption', string = regexs)\n",
    "    if tags:\n",
    "        for tag in tags:\n",
    "            tag.parent.decompose()\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Renaming labels\n",
    "\n",
    "Consider this label:\n",
    "\n",
    "\n",
    "Since this label changes every page depending on the city name, we'll want to standardize it by renaming it to 'City Population'.\n",
    "\n",
    "\n",
    "We'll also want to rename these labels to remove ambiguity:\n",
    "\n",
    "\n",
    "On this table, we'll want to add some prefixes to avoid ambiguity with other fields:\n",
    "\n",
    "\n",
    "These labels need gender suffixes to avoid clashing:\n",
    "\n",
    "\n",
    "The awards section needs serious relabelling:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rename_labels(soup):\n",
    "    \n",
    "    # Relabel the varying '{city_name} Population' tag with a constant label.\n",
    "    tag = soup.find(string = re.compile('Population'))\n",
    "    if tag:\n",
    "        tag.string = 'City Population'\n",
    "        \n",
    "    # Relabel some labels on first page to remove ambiguity with duplicates.\n",
    "    tag = soup.find('th', string = 'Undergraduate Students')\n",
    "    if tag:\n",
    "        women = tag.find_next('th', string = re.compile('Women'))\n",
    "        men = tag.find_next('th', string = re.compile('Men'))\n",
    "        grads = tag.find_next('th', string = re.compile('Graduate Students'))\n",
    "        \n",
    "        tag.string = 'All Undergraduates'\n",
    "        women.string = 'Undergrads (women)'\n",
    "        men.string = 'Undergrads (men)'\n",
    "        grads.string = 'All Graduate Students'\n",
    "        \n",
    "    # Add prefix to table labels to remove ambiguity with other fields.\n",
    "    tag = soup.find('div', id = 'section7')\n",
    "    if tag:\n",
    "        th_tags = tag.table.tbody.find_all('th')\n",
    "        for th_tag in th_tags:\n",
    "            label = \" \".join(th_tag.stripped_strings)\n",
    "            th_tag.string = 'Factor - ' + label\n",
    "            \n",
    "    # Add gender suffixes to duplicate field names.\n",
    "    tag = soup.find('div', id = 'section8')\n",
    "    if tag:\n",
    "        adm_rate_w = tag.find_next('th', string = re.compile('Women'))\n",
    "        adm_rate_m = tag.find_next('th', string = re.compile('Men'))\n",
    "        enrolled_w = adm_rate_w.find_next('th', string = re.compile('Women'))\n",
    "        enrolled_m = adm_rate_m.find_next('th', string = re.compile('Men'))\n",
    "        \n",
    "        adm_rate_w.string = 'Overall Admission Rate (women)'\n",
    "        adm_rate_m.string = 'Overall Admission Rate (men)'\n",
    "        enrolled_w.string = 'Students Enrolled (women)'\n",
    "        enrolled_m.string = 'Students Enrolled (men)'\n",
    "\n",
    "    # Add appropriate markup to ambiguous need-based award labels.\n",
    "    div_tag = soup.find('div', id = 'section11')\n",
    "    if div_tag:\n",
    "        captions = ['Freshmen', 'All Undergraduates']\n",
    "        for caption in captions:\n",
    "            cap_tag = div_tag.find('caption', string = re.compile(caption))\n",
    "            table_tag = cap_tag.parent\n",
    "            tags = table_tag.tbody.find_all('th')\n",
    "            for tag in tags:\n",
    "                tag.string = tag.string + ' (' + caption + ')'\n",
    "                if tag.attrs == {'class': ['sub']}:\n",
    "                    tag.string = 'Average Award - ' + tag.string\n",
    "                    \n",
    "    # Add appropriate markup to ambiguous non-need based award labels.\n",
    "    div_tag = soup.find('div', id = 'section12')\n",
    "    if div_tag:\n",
    "        caption = re.compile('Non-Need Awards')\n",
    "        cap_tag = div_tag.find('caption', string = caption)\n",
    "        table_tag = cap_tag.parent\n",
    "        tags = table_tag.tbody.find_all('th')\n",
    "        for tag in tags:\n",
    "            if tag.attrs != {'class': ['sub']}:\n",
    "                tag.string = \" \".join(tag.stripped_strings)\n",
    "                subtags = tag.find_all_next('th')[:2]\n",
    "                for subtag in subtags:\n",
    "                    subtag.string = \" \".join(subtag.stripped_strings)\n",
    "                    subtag.string = tag.string[:-12] + \" - \" + subtag.string\n",
    "    \n",
    "\n",
    "            \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Scraping rows\n",
    "\n",
    "With our soup finally cleaned, we're ready to scrape all the rows.\n",
    "\n",
    "First, we'll save the `<h1>` header string as the 'Name' of the school.\n",
    "\n",
    "We'll descend into the `tabcontwrap` `<div>` part of the page, which contains all the data.\n",
    "\n",
    "We'll delete any tables (since we're only scraping data from rows).\n",
    "\n",
    "For all remaining `<th>` headers and `<td>` data tags, we'll compress their potentially complicated and split up string contents into a single simple string.\n",
    "\n",
    "Then for each `<tr>` row they are in, we'll set the `<th>` string as the label and the `<td>` string as the value and save them to the rows dictionary, which we'll return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_rows(soup):\n",
    "    rows = {}\n",
    "    \n",
    "    if soup:\n",
    "\n",
    "        for tag in soup('thead'):\n",
    "            tag.parent.decompose()\n",
    "\n",
    "        for tag in soup(['th','td']):\n",
    "            tag.string = \" \".join(tag.stripped_strings)\n",
    "\n",
    "        for tr in soup('tr'):\n",
    "            if tr('th') and tr('td'):\n",
    "                label = tr.find('th').string\n",
    "                value = tr.find('td').string\n",
    "                rows[label] = value\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Scraping tables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_tables(soup):\n",
    "    tables = {}\n",
    "    if soup:\n",
    "\n",
    "        content = soup.find('div', id = 'tabcontwrap')\n",
    "        for thead in content('thead'):\n",
    "\n",
    "            # Get column labels\n",
    "            td_tags = thead('td')\n",
    "            col_labels = []\n",
    "            for i, td_tag in enumerate(td_tags):\n",
    "                label = \" \".join(td_tag.stripped_strings)\n",
    "                col_labels.append(label)\n",
    "\n",
    "            # Get row labels and cell values.\n",
    "            table_values = {}\n",
    "            tr_tags = thead.parent.tbody('tr')\n",
    "            for tr_tag in tr_tags:\n",
    "                \n",
    "                # Get the row label.\n",
    "                row_label = \" \".join(tr_tag.th.stripped_strings)\n",
    "                \n",
    "                if row_label:\n",
    "                    # Get the row values.\n",
    "                    row_values = []\n",
    "                    td_tags = tr_tag('td')\n",
    "                    for td_tag in td_tags:\n",
    "                        row_values.append(\" \".join(td_tag.stripped_strings))\n",
    "\n",
    "                    # Determine if row val should be saved as categorical var.\n",
    "                    unique_vals = set(row_values)\n",
    "                    if (len(unique_vals) == 2) and ('X' in unique_vals):\n",
    "                        index_val = row_values.index('X')\n",
    "                        label = row_label\n",
    "                        table_values[label] = col_labels[index_val]\n",
    "\n",
    "                    # Or else, append the column label to the row label.\n",
    "                    else:\n",
    "                        for j, row_value in enumerate(row_values):\n",
    "                            label = row_label\n",
    "                            if col_labels[j]:\n",
    "                                label = label + \" - \" + col_labels[j]\n",
    "                            table_values[label] = row_value\n",
    "            \n",
    "            tables.update(table_values)\n",
    "\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://www.collegedata.com/cs/data/college/college_pg01_tmpl.jhtml?schoolId=5000\n"
     ]
    }
   ],
   "source": [
    "scrape_collegedata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1400 entries, 0 to 1399\n",
      "Data columns (total 3 columns):\n",
      "Name         1400 non-null object\n",
      "Location     1400 non-null object\n",
      "Rank Info    1400 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 32.9+ KB\n"
     ]
    }
   ],
   "source": [
    "USNEWS_HTML_PATH = 'data/usnews.html'\n",
    "USNEWS_RAW_CSV_PATH = 'data/usnews_raw.csv'\n",
    "ROW_ID_ATTR = {\"data-view\":\"colleges-search-results-table-row\"}\n",
    "\n",
    "scraped = []\n",
    "with open(USNEWS_HTML_PATH, 'r') as file:\n",
    "    page = BeautifulSoup(file.read(), \"lxml\")\n",
    "    for row in page('tr', attrs = ROW_ID_ATTR):\n",
    "        vals = \"---\".join(row.stripped_strings).split('---')\n",
    "        if '(tie)' in vals:\n",
    "            vals.remove('(tie)')\n",
    "        if '1' in vals:\n",
    "            vals.remove('1')\n",
    "        scraped.append(vals[0:3])\n",
    "        \n",
    "ranks_df = pd.DataFrame(scraped, columns = ['Name','Location','Rank Info'])\n",
    "ranks_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the raw scraped rankings to a .csv for later cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ranks_df.to_csv(USNEWS_RAW_CSV_PATH, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
