{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from os.path import isfile\n",
    "from requests import get\n",
    "from IPython.core.display import clear_output\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14) \"\n",
    "           \"AppleWebKit/605.1.15 (KHTML, like Gecko) \"\n",
    "           \"Version/12.0 Safari/605.1.15\"}\n",
    "\n",
    "BASE_URL_1 = \"https://www.collegedata.com/cs/data/college/college_pg0\"\n",
    "BASE_URL_2 = \"_tmpl.jhtml?schoolId=\"\n",
    "\n",
    "EMPTY_H1_HEADING = \"Retrieve a Saved Search\"\n",
    "\n",
    "SCHOOL_ID_START = 1\n",
    "SCHOOL_ID_END = 5000\n",
    "\n",
    "PATH = \"data/test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_collegedata(start = SCHOOL_ID_START, stop = SCHOOL_ID_END):\n",
    "    for school_id in range(start, stop + 1):\n",
    "        school = scrape_school(school_id)\n",
    "        if school:\n",
    "            df = pd.DataFrame(school, index = [school_id])\n",
    "            df.to_csv(PATH, mode = 'a+', header = ~isfile(PATH), \n",
    "                      index_label = 'SchoolId')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def scrape_school(school_id):\n",
    "    school = {}\n",
    "    for page_id in range(1, 7):\n",
    "        page = scrape_page(school_id, page_id)\n",
    "        if not page:\n",
    "            break\n",
    "        school.update(page)\n",
    "    return school"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_page(school_id, page_id):\n",
    "    soup = get_soup(school_id, page_id)\n",
    "    cleaned_soup = clean_soup(soup)\n",
    "    \n",
    "    page = {}\n",
    "    page.update(scrape_rows(cleaned_soup))\n",
    "    page.update(scrape_tables(cleaned_soup))\n",
    "    return page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_soup(school_id, page_id):\n",
    "    \n",
    "    # Build URL and send request.\n",
    "    url = BASE_URL_1 + str(page_id) + BASE_URL_2 + str(school_id)\n",
    "    print(\"Scraping {}\".format(url))\n",
    "    result = get(url, headers = HEADERS)\n",
    "    clear_output(wait = True)\n",
    "    \n",
    "    # Abort if received an unusual status code.\n",
    "    if result.status_code != 200:\n",
    "        # TO DO: Add a proper way to handle or at least record this error.\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(result.text, \"lxml\")\n",
    "    \n",
    "    # Abort if page does not have a <h1> header.\n",
    "    if not soup.h1:\n",
    "        # TO DO: Add a proper way to handle or at least record this error.\n",
    "        return None\n",
    "    \n",
    "    # Abort if <h1> header matches that of the error page with no school data.\n",
    "    if soup.h1.string == EMPTY_H1_HEADING:\n",
    "        # TO DO: Add a proper way to handle or at least record this error.\n",
    "        return None\n",
    "    \n",
    "    # Abort if the entire page HTML did not load.\n",
    "    if not soup.find(string = 'Content END'):\n",
    "        # TO DO: Add a proper way to handle or at least record this error.\n",
    "        return None\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_soup(soup):\n",
    "    \n",
    "    if not soup:\n",
    "        return None\n",
    "        \n",
    "    # Delete certain div sections by their id tags.\n",
    "    div_ids = ['section19']\n",
    "    tags = soup.find_all(id = div_ids)\n",
    "    if tags:\n",
    "        for tag in tags:\n",
    "            tag.decompose()\n",
    "    \n",
    "    # Delete certain redundant tables by their caption strings.\n",
    "    captions = ['Selection of Students',\n",
    "                'Grade Point Average of Enrolled Freshmen',\n",
    "                'SAT Scores of Enrolled Freshmen',\n",
    "                'ACT Scores of Enrolled Freshmen',\n",
    "                'Financial Aid Office',\n",
    "                'Undergraduate Majors',\n",
    "                'Intercollegiate Sports Offered']\n",
    "    regexs = [re.compile(caption) for caption in captions]\n",
    "    tags = soup.find_all('caption', string = regexs)\n",
    "    if tags:\n",
    "        for tag in tags:\n",
    "            tag.parent.decompose()\n",
    "    \n",
    "    # Relabel the varying '{city_name} Population' tag with a constant label.\n",
    "    tag = soup.find(string = re.compile('Population'))\n",
    "    if tag:\n",
    "        tag.string = 'City Population'\n",
    "        \n",
    "    # Relabel some labels on first page to remove ambiguity with duplicates.\n",
    "    tag = soup.find('th', string = 'Undergraduate Students')\n",
    "    if tag:\n",
    "        women = tag.find_next('th', string = re.compile('Women'))\n",
    "        men = tag.find_next('th', string = re.compile('Men'))\n",
    "        grads = tag.find_next('th', string = re.compile('Graduate Students'))\n",
    "        \n",
    "        tag.string = 'All Undergraduates'\n",
    "        women.string = 'Undergrads (women)'\n",
    "        men.string = 'Undergrads (men)'\n",
    "        grads.string = 'All Graduate Students'\n",
    "        \n",
    "    # Add prefix to table labels to remove ambiguity with other fields.\n",
    "    tag = soup.find('div', id = 'section7')\n",
    "    if tag:\n",
    "        th_tags = tag.table.tbody.find_all('th')\n",
    "        for th_tag in th_tags:\n",
    "            label = \" \".join(th_tag.stripped_strings)\n",
    "            th_tag.string = 'Factor - ' + label\n",
    "            \n",
    "    # Add gender suffixes to duplicate field names.\n",
    "    tag = soup.find('div', id = 'section8')\n",
    "    if tag:\n",
    "        adm_rate_w = tag.find_next('th', string = re.compile('Women'))\n",
    "        adm_rate_m = tag.find_next('th', string = re.compile('Men'))\n",
    "        enrolled_w = adm_rate_w.find_next('th', string = re.compile('Women'))\n",
    "        enrolled_m = adm_rate_m.find_next('th', string = re.compile('Men'))\n",
    "        \n",
    "        adm_rate_w.string = 'Overall Admission Rate (women)'\n",
    "        adm_rate_m.string = 'Overall Admission Rate (men)'\n",
    "        enrolled_w.string = 'Students Enrolled (women)'\n",
    "        enrolled_m.string = 'Students Enrolled (men)'\n",
    "\n",
    "    # Add appropriate markup to ambiguous need-based award labels.\n",
    "    div_tag = soup.find('div', id = 'section11')\n",
    "    if div_tag:\n",
    "        captions = ['Freshmen', 'All Undergraduates']\n",
    "        for caption in captions:\n",
    "            cap_tag = div_tag.find('caption', string = re.compile(caption))\n",
    "            table_tag = cap_tag.parent\n",
    "            tags = table_tag.tbody.find_all('th')\n",
    "            for tag in tags:\n",
    "                tag.string = tag.string + ' (' + caption + ')'\n",
    "                if tag.attrs == {'class': ['sub']}:\n",
    "                    tag.string = 'Average Award - ' + tag.string\n",
    "                    \n",
    "    # Add appropriate markup to ambiguous non-need based award labels.\n",
    "    div_tag = soup.find('div', id = 'section12')\n",
    "    if div_tag:\n",
    "        caption = re.compile('Non-Need Awards')\n",
    "        cap_tag = div_tag.find('caption', string = caption)\n",
    "        table_tag = cap_tag.parent\n",
    "        tags = table_tag.tbody.find_all('th')\n",
    "        for tag in tags:\n",
    "            if tag.attrs != {'class': ['sub']}:\n",
    "                tag.string = \" \".join(tag.stripped_strings)\n",
    "                subtags = tag.find_all_next('th')[:2]\n",
    "                for subtag in subtags:\n",
    "                    subtag.string = \" \".join(subtag.stripped_strings)\n",
    "                    subtag.string = tag.string[:-12] + \" - \" + subtag.string\n",
    "    \n",
    "    # Delete duplicate values.\n",
    "    tag = soup.find('div', id = 'section26')\n",
    "    if tag:\n",
    "        strings = ['All Undergraduates','Women','Men']\n",
    "        for string in strings:\n",
    "            regex = re.compile(string)\n",
    "            tag.find_next('th', string = regex).parent.decompose()\n",
    "            \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrape_rows(soup):\n",
    "    rows = {}\n",
    "    if soup:\n",
    "        rows['Name'] = soup.h1.string\n",
    "        \n",
    "        content = soup.find('div', id = 'tabcontwrap')\n",
    "\n",
    "        for tag in content('thead'):\n",
    "            tag.parent.decompose()\n",
    "\n",
    "        for tag in content(['th','td']):\n",
    "            tag.string = \" \".join(tag.stripped_strings)\n",
    "\n",
    "        for tr in content('tr'):\n",
    "            if tr('th') and tr('td'):\n",
    "                label = tr.find('th').string\n",
    "                value = tr.find('td').string\n",
    "                rows[label] = value\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tables(soup):\n",
    "    tables = {}\n",
    "    if soup:\n",
    "\n",
    "        content = soup.find('div', id = 'tabcontwrap')\n",
    "        for thead in content('thead'):\n",
    "\n",
    "            # Get column labels\n",
    "            td_tags = thead('td')\n",
    "            col_labels = []\n",
    "            for i, td_tag in enumerate(td_tags):\n",
    "                label = \" \".join(td_tag.stripped_strings)\n",
    "                col_labels.append(label)\n",
    "\n",
    "            # Get row labels and cell values.\n",
    "            table_values = {}\n",
    "            tr_tags = thead.parent.tbody('tr')\n",
    "            for tr_tag in tr_tags:\n",
    "                \n",
    "                # Get the row label.\n",
    "                row_label = \" \".join(tr_tag.th.stripped_strings)\n",
    "                \n",
    "                if row_label:\n",
    "                    # Get the row values.\n",
    "                    row_values = []\n",
    "                    td_tags = tr_tag('td')\n",
    "                    for td_tag in td_tags:\n",
    "                        row_values.append(\" \".join(td_tag.stripped_strings))\n",
    "\n",
    "                    # Determine if row val should be saved as categorical var.\n",
    "                    unique_vals = set(row_values)\n",
    "                    if (len(unique_vals) == 2) and ('X' in unique_vals):\n",
    "                        index_val = row_values.index('X')\n",
    "                        label = row_label\n",
    "                        table_values[label] = col_labels[index_val]\n",
    "\n",
    "                    # Or else, append the column label to the row label.\n",
    "                    else:\n",
    "                        for j, row_value in enumerate(row_values):\n",
    "                            label = row_label\n",
    "                            if col_labels[j]:\n",
    "                                label = label + \" - \" + col_labels[j]\n",
    "                            table_values[label] = row_value\n",
    "            \n",
    "            tables.update(table_values)\n",
    "\n",
    "    return tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scrape_collegedata()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
