{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from os.path import isfile\n",
    "from requests import get\n",
    "from IPython.core.display import clear_output\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "# Use this tool to scrape CollegeData.com into a CSV.\n",
    "#############################################################################\n",
    "# The URLs of interest on collegedata.com are broken down like this:\n",
    "BASE_URL_1 = \"https://www.collegedata.com/cs/data/college/college_pg0\"\n",
    "# followed by a `school_id` number. Following that, there is:\n",
    "BASE_URL_2 = \"_tmpl.jhtml?schoolId=\"\n",
    "# Finally, there is a `page_id` number, ranging from 1 to 6, inclusive.\n",
    "# Not all possible `school_id` numbers corresponds to a school, but most do.\n",
    "# A page requested corresponding to a `school_id` with no school data will\n",
    "# load a page that has a <h1> tag heading with this string:\n",
    "EMPTY_H1_HEADING = \"Retrieve a Saved Search\"\n",
    "# At larger `school_id` values, especially over 1000, no-school pages are\n",
    "# returned more often. I'm fairly confident there are none over 5000.\n",
    "SCHOOL_ID_START = 1\n",
    "SCHOOL_ID_END = 10\n",
    "# CollegeData.com has no problem with requests without headers, but we can\n",
    "# send a fake header anyway:\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14) \"\n",
    "           \"AppleWebKit/605.1.15 (KHTML, like Gecko) \"\n",
    "           \"Version/12.0 Safari/605.1.15\"}\n",
    "# Finally, we'll export our scraped data to a CSV. The scraper will check\n",
    "# if the CSV already exists, and if so, will adjust the `SCHOOL_ID_START` to\n",
    "# begin with the `school_id` after the highest already scraped in the CSV:\n",
    "PATH = \"test_scraped.csv\"\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "# Exceptions\n",
    "#############################################################################\n",
    "# These errors could arise while requesting and scraping the pages.\n",
    "class PageRequestError(Exception):\n",
    "    # The response code was something other than 200.\n",
    "    pass\n",
    "\n",
    "class NoHeadingError(Exception):\n",
    "    # The HTML has no <h1> tag.\n",
    "    pass\n",
    "\n",
    "class NoSchoolError(Exception):\n",
    "    # There is no school data for the given `school_id`. Not really an error.\n",
    "    pass\n",
    "\n",
    "class IncompletePageError(Exception):\n",
    "    # The comment code at the bottom of a fully loaded page was not found.\n",
    "    pass\n",
    "\n",
    "class EmptyPageError(Exception):\n",
    "    # No data was scraped from a fully loaded normal page.\n",
    "    pass\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "# Functions\n",
    "#############################################################################\n",
    "def scrape_collegedata(start = SCHOOL_ID_START, stop = SCHOOL_ID_END):\n",
    "    \"\"\"Extract data from CollegeData.com.\n",
    "\n",
    "    Keyword arguments:\n",
    "    start -- `schoolId` in URL to begin scraping, inclusive.\n",
    "    stop  -- `schoolId` in URL to end scraping, inclusive.\n",
    "\n",
    "    If data for a `schoolId` exists, scrape_collegedata will request\n",
    "    all six pages (Overview, Admissions, Money Matters, Academics, Campus\n",
    "    Life, and Students) associated with that school. Relevant rows and tables\n",
    "    will be scraped from the six pages and saved together as a single pandas \n",
    "    Series, with the `schoolId` as the Series' name. \n",
    "\n",
    "    scrape_collegedata will return a list of Series, one for each\n",
    "    scraped school.\n",
    "     \"\"\"\n",
    "\n",
    "    # Hold scraped schools as a list of pandas series.\n",
    "    scraped_list = []\n",
    "\n",
    "    # Scrape and save each page of each school.\n",
    "    for school_id in range(start, stop + 1):\n",
    "        try:\n",
    "            # Request and parse HTML for all six pages associated w/ a school_id.\n",
    "            soups = [get_soup(school_id, page_id) for page_id in range(1, 7)]\n",
    "            \n",
    "            # Relabel some problematic <th> labels before scraping.\n",
    "            soups = relabel(soups)\n",
    "            \n",
    "            # School dictionary to hold all scraped values.\n",
    "            school_dict = extract(soups)\n",
    "\n",
    "        except PageRequestError:\n",
    "            print('PageRequestError: Status Code != 200')\n",
    "        except NoHeadingError:\n",
    "            print('NoHeadingError: Page HTML has no <h1> tag.')\n",
    "        except NoSchoolError:\n",
    "            print('NoSchoolError: Page intentionally has no school data.')\n",
    "        except IncompletePageError:\n",
    "            print('IncompletePageError: Page did not completely load.')\n",
    "        except EmptyPageError:\n",
    "            print('EmptyPageError: No data scraped from page.')\n",
    "        else:\n",
    "            # Convert dict to pandas Series and append to scaped_list.\n",
    "            scraped_series = pd.Series(school_dict, name = school_id)\n",
    "            scraped_list.append(scraped_series)\n",
    "        finally:\n",
    "            # Clear status message.\n",
    "            clear_output()\n",
    "\n",
    "    return scraped_list\n",
    "\n",
    "\n",
    "def get_soup(school_id, page_id):\n",
    "    # Build the CollegeData.com URL corresponding to school_id and page_id.\n",
    "    url = BASE_URL_1 + str(page_id) + BASE_URL_2 + str(school_id)\n",
    "\n",
    "    # Request page and check if page returned.\n",
    "    result = get(url, headers = HEADERS)\n",
    "    if result.status_code != 200:\n",
    "        raise PageRequestError\n",
    "        \n",
    "    # Parse page text into BeautifulSoup object.\n",
    "    strainer = SoupStrainer(lambda tag, d: tag == 'h1' or d.get('id') == 'tabcontwrap')\n",
    "    soup = BeautifulSoup(result.text, \"lxml\", parse_only = strainer)\n",
    "    \n",
    "    # Raise an error if no <h1> tag exists.\n",
    "    if not soup.h1:\n",
    "        raise NoHeadingError\n",
    "\n",
    "    # Raise an error if the <h1> text suggests no school exists.\n",
    "    if soup.h1.string == EMPTY_H1_HEADING:\n",
    "        raise NoSchoolError\n",
    "\n",
    "    # Raise an error if the HTML comment at the end of the page content\n",
    "    # didn't load, which can sometimes happen when requesting too fast.\n",
    "    #if not soup.find(string = 'Content END'):\n",
    "    #    raise IncompletePageError\n",
    "        \n",
    "    # Print a status update.\n",
    "    print('Scraped school {}, page {}'.format(school_id, page_id))\n",
    "\n",
    "    return soup\n",
    "\n",
    "\n",
    "def relabel(soups):\n",
    "    # Prepend headings and clean repeated gender labels.\n",
    "    strings = ['\\xa0\\xa0 Women', '\\xa0\\xa0 Men']\n",
    "    tr_list = [[tr for soup in soups for tr in soup('tr') \\\n",
    "                if re.search(string, tr.get_text())] for string in strings]\n",
    "    headings = [tr.find_previous_sibling('tr').th.get_text() for tr in tr_list[0]]*2\n",
    "    th_list = [tr.th for trs in tr_list for tr in trs] # flatten list\n",
    "    labels = [th.get_text(strip = True) for th in th_list]\n",
    "    _ = [th.string.replace_with(headings[i] + ', ' + labels[i]) \\\n",
    "                for i, th in enumerate(th_list)]\n",
    "    \n",
    "    # Prepend heading to repeated phone and email labels.\n",
    "    heading = 'Financial Aid Office'\n",
    "    table = soups[2].find('caption', string = re.compile(heading)).parent\n",
    "    _ = [th.string.replace_with(heading + ', ' + th.string) \\\n",
    "         for th in table.tbody('th')[0:2]]\n",
    "    \n",
    "    # Append captions to duplicate financial aid labels.\n",
    "    tables = soups[2].find('div', id = 'section11')('table')[0:2]\n",
    "    captions = [table.caption.get_text(strip = True) for table in tables]\n",
    "    tr_list = [table.tbody('tr') for table in tables]\n",
    "    th_list = [[tr.th for tr in trs] for trs in tr_list]\n",
    "    _ = [[th.insert(1, ', ' + captions[i]) \\\n",
    "                for th in ths if not th.has_attr('class')] \\\n",
    "                    for i, ths in enumerate(th_list)]\n",
    "    \n",
    "    # Prepend a heading to the GPA labels.\n",
    "    regex = re.compile(\"Grade Point Average\")\n",
    "    table = soups[1].find('caption', string = regex).parent\n",
    "    _ = table.tbody.th.string.replace_with('GPA, Average')\n",
    "    _ = [th.insert(0, 'GPA, ') for th in table.tbody('th')[1:]]\n",
    "    \n",
    "    return soups\n",
    "\n",
    "\n",
    "def extract(soups):   \n",
    "    extracted_dict = {}\n",
    "    \n",
    "    # Extract the school name.\n",
    "    #h1 = soups[0].h1.extract()\n",
    "    #d['Name'] = h1.get_text()\n",
    "    extracted_dict.update(extract_name())\n",
    "    \n",
    "    # Extract the fafsa code and filing cost.\n",
    "    regex = re.compile('Forms Required')\n",
    "    table = soups[2].find('th', string = regex).find_parent('table').extract()\n",
    "    extracted_dict['FAFSA'] = table.tbody.th.get_text()\n",
    "    extracted_dict['Financial Aid Filing Cost'] = table.tbody.td.get_text()\n",
    "    #d.update(extract_applying_finaid())\n",
    "\n",
    "    # Extract the city population.\n",
    "    regex = re.compile('Population')\n",
    "    tr = soups[0].find('th', string = regex).find_parent('tr').extract()\n",
    "    _ = soups[4].find('th', string = regex).find_parent('tr').decompose()\n",
    "    extracted_dict['City Population'] = tr.td.get_text()\n",
    "    #d.update(extract_city_pop())\n",
    "    \n",
    "    # Extract <th class='sub'> rows with appropriate header labels prepended to keys.\n",
    "    tr_list = [[th.parent for th in soup('th', class_ = 'sub')] for soup in soups]\n",
    "    tr_list = [tr for sublist in tr_list for tr in sublist]  # flatten list\n",
    "    sub_keys = [tr.th.get_text(strip = True) for tr in tr_list]\n",
    "    headers = [tr.find_previous('th', class_ = None).get_text() for tr in tr_list]\n",
    "    keys = [headers[i] + ', ' + sub_key for i, sub_key in enumerate(sub_keys)]\n",
    "    vals = [tr.extract().td.get_text(strip = True) for tr in tr_list]\n",
    "    extracted_dict.update(dict(zip(keys, vals)))\n",
    "    #d.update(extract_sub_rows())\n",
    "\n",
    "    # Extract from 'Selection of Students' table.\n",
    "    table = soups[1].find('div', id = 'section7').table.extract()\n",
    "    col_labels = [td.get_text() for td in table.thead.tr('td')]\n",
    "    rows = [tr for tr in table.tbody('tr') if 'X' in [td.get_text() for td in tr('td')]]\n",
    "    index = [[td.get_text() for td in tr('td')].index('X') for tr in rows]\n",
    "    keys = ['Selection Factor, ' + tr.th.get_text() for tr in rows]\n",
    "    vals = [col_labels[i] for i in index]\n",
    "    extracted_dict.update(dict(zip(keys, vals)))\n",
    "    #d.update(extract_table_selection_students())\n",
    "\n",
    "    # Extract from majors / programs of study tables.\n",
    "    keys = ['Undergraduate Majors',\n",
    "            \"Master's Programs of Study\",\n",
    "            'Doctoral Programs of Study']\n",
    "    regexs = [re.compile(key) for key in keys]\n",
    "    tags = soups[3]('caption', string = regexs)\n",
    "    tables = [tag.find_parent('table').extract() for tag in tags]\n",
    "    vals = [[li.get_text(strip = True) for li in table('li')] for table in tables]\n",
    "    extracted_dict.update(dict(zip(keys, vals)))\n",
    "    #d.update(extract_table_majors())\n",
    "\n",
    "    # Extract from master's / doctoral degrees tables.\n",
    "    keys = [\"Master's Degrees Offered\",\n",
    "            \"Doctoral Degrees Offered\"]\n",
    "    regexs = [re.compile(key) for key in keys]\n",
    "    tags = soups[3]('caption', string = regexs)\n",
    "    tables = [tag.find_parent('table').extract() for tag in tags]\n",
    "    vals = [table.th.get_text(strip = True).split(\", \") for table in tables]\n",
    "    extracted_dict.update(dict(zip(keys, vals)))\n",
    "    #d.update(extract_table_degrees())\n",
    "\n",
    "    # TO DO: Extract remaining tables (HS units, sports).\n",
    "\n",
    "    # Finally, extract all <th> as key, <td> as value data from key/val rows.\n",
    "    rows = [row.extract() for soup in soups for row in soup('tr') \\\n",
    "                if len(row('th')) == 1 and len(row('td')) == 1]\n",
    "    keys = [tr.th.get_text(strip = True) for tr in rows]\n",
    "    vals = [tr.td.get_text(strip = True) for tr in rows]\n",
    "    extracted_dict.update(dict(zip(keys, vals)))\n",
    "    #d.update(extract_rows())\n",
    "\n",
    "    # Return scraped data as a dictionary.\n",
    "    return extracted_dict\n",
    "\n",
    "\n",
    "def extract_name():\n",
    "    d = {}\n",
    "        \n",
    "    h1 = soups[0].h1.extract()\n",
    "    d['Name'] = h1.get_text()\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped school 59, page 1\n",
      "Scraped school 59, page 2\n",
      "Scraped school 59, page 3\n",
      "Scraped school 59, page 4\n",
      "Scraped school 59, page 5\n",
      "Scraped school 59, page 6\n"
     ]
    }
   ],
   "source": [
    "soups = [get_soup(59, page_id) for page_id in range(1, 7)]\n",
    "\n",
    "title = soups[0].find('h1').extract().get_text()\n",
    "desc = soups[0].find('p').extract().get_text()\n",
    "_ = [h1.decompose() for soup in soups for h1 in soup('h1')]\n",
    "\n",
    "contents = [soup.find('div', id = 'tabcontwrap').extract() for soup in soups]\n",
    "_ = [div.attrs.update({'id':'page' + str(i)}) for i, div in enumerate(contents, start = 1)]\n",
    "\n",
    "soup = BeautifulSoup()\n",
    "_ = [soup.insert(0, content) for content in reversed(contents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [table.extract() for table in soup('table')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '1'},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '1'},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '1', 'class': ['laligntable']},\n",
       " {'cellspacing': '1', 'class': ['laligntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '1'},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '1', 'style': 'margin-top:10px;', 'class': ['laligntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '1'},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']},\n",
       " {'cellspacing': '0', 'class': ['onecolumntable']}]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[table.attrs.update({'class':'other'}) for table in tables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False, False, False, False]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sections = {div.get('id') : div.extract() for soup in soups for div in soup('div') if div.get('id') and re.match('section\\d+', div.get('id'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['section0', 'section1', 'section2', 'section3', 'section4', 'section5', 'section6', 'section7', 'section8', 'section9', 'section10', 'section11', 'section12', 'section13', 'section14', 'section15', 'section16', 'section17', 'section18', 'section19', 'section20', 'section21', 'section22', 'section23', 'section24', 'section25', 'section26', 'section27', 'section28'])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_tables = [table.extract() for soup in soups for table in soup('table')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"openedsection\" id=\"section0\">\n",
       "<table cellspacing=\"0\" class=\"onecolumntable\">\n",
       "<colgroup>\n",
       "<col class=\"namecol\"/>\n",
       "</colgroup>\n",
       "<tbody>\n",
       "<tr>\n",
       "<th>Entrance Difficulty</th><td>Very difficult</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<th>Overall Admission Rate</th><td>50% of 56,114 applicants were admitted</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<th>Early Action Offered</th><td>No</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<th>Early Decision Offered</th><td>No</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<th>Regular Admission Deadline</th><td>Rolling</td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>\n",
       "<table cellspacing=\"1\">\n",
       "<caption>\n",
       "\t\t\t  Selection of Students\n",
       "\t\t\t</caption>\n",
       "<colgroup>\n",
       "<col/>\n",
       "<col class=\"selectioncol\" span=\"4\"/>\n",
       "</colgroup>\n",
       "<thead>\n",
       "<tr>\n",
       "<th>Factor</th><td>Very Important</td><td>Important</td><td>Considered</td><td>Not Considered</td>\n",
       "</tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr>\n",
       "<th>Academic GPA</th><td>X</td><td></td><td></td><td></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<th>Standardized Tests</th><td>X</td><td></td><td></td><td></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<th>Essay</th><td></td><td></td><td>X</td><td></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<th>Extracurricular Activities</th><td></td><td></td><td>X</td><td></td>\n",
       "</tr>\n",
       "<tr>\n",
       "<th class=\"sub\"><div><a class=\"more\" href='javascript:viewCollegeProfile(\"https://www.collegedata.com/cs/data/college/college_pg02_tmpl.jhtml\", \"#selectionOfStudents\")' title=\"More Factors\">More Factors...</a></div></th><td></td><td></td><td></td><td></td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>\n",
       "<table cellspacing=\"0\" class=\"onecolumntable\">\n",
       "<caption>\n",
       "\t\t\t  Qualifications of Enrolled Freshmen\n",
       "\t\t\t</caption>\n",
       "<colgroup>\n",
       "<col class=\"namecol\"/>\n",
       "</colgroup>\n",
       "<tbody>\n",
       "<tr>\n",
       "<th>Average GPA</th><td>3.58</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<th>SAT Math</th><td>629 average <br/>580-680 range of middle 50%</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<th>SAT Critical Reading</th><td>618 average <br/>580-660 range of middle 50%</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<th>SAT Writing</th><td>Not reported</td>\n",
       "</tr>\n",
       "<tr>\n",
       "<th>ACT Composite</th><td>28 average <br/>25-30 range of middle 50%</td>\n",
       "</tr>\n",
       "</tbody>\n",
       "</table>\n",
       "<div class=\"overviewlink\">\n",
       "<a href='javascript:viewCollegeProfile(\"https://www.collegedata.com/cs/data/college/college_pg02_tmpl.jhtml\")' id=\"ml_admission\">More Admission</a>\n",
       "</div>\n",
       "</div>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
